{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"center\">\n",
        "    <img src=\"https://raw.githubusercontent.com/GabrieleAraujo/database-data-mining/main/icon/Ufopa_braso_PNG_fundo_transparente.png\" width=\"15%\">\n",
        "    <img src=\"https://raw.githubusercontent.com/GabrieleAraujo/database-data-mining/main/icon/LACA_Logo_HQ_Minimalista.png\" width=\"15%\">\n",
        "</div>\n",
        "\n",
        "## Market Basket Analysis in Data Mining\n",
        "#### Teacher:  Fábio Lobato - https://fabiolobato.github.io/\n",
        "#### Assistant: Gabriele Araújo - [Linkedin](https://www.linkedin.com/in/gabrielesaraujo/)\n",
        "#### Federal University of Western Pará\n",
        "\n",
        "<!-- </br>\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/GabrieleAraujo/database-data-mining/main/icon/members-grep.comp4.png\" width=\"100%\">\n",
        "\n",
        "</br> -->\n",
        "\n",
        "***\n",
        "\n",
        "\n",
        "\n",
        "Repositories used for inspiration of this material: </br>\n",
        "- [Market-basket-analysis - Xavier](https://www.kaggle.com/code/xvivancos/market-basket-analysis/)\n",
        "- [Market-basket-analysis - Nagadia](https://www.kaggle.com/code/meetnagadia/market-basket-analysis/)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "veOc0FSZjxkK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evoJZ9vhLIZT"
      },
      "source": [
        "# Market Basket Analysis\n",
        "\n",
        "*Market Basket Analysis* é uma técnica de data mining que faz uso de regras de associação para identificar os hábitos de compra dos clientes, fornecendo uma visão da combinação de produtos dentro das cestas de compras dos clientes analisados.\n",
        "\n",
        "As regras de associação têm como premissa básica encontrar elementos que implicam na presença de outros elementos em uma mesma transação, ou seja, encontrar relacionamentos ou padrões frequentes entre conjuntos de dados.\n",
        "\n",
        "\n",
        "<center>\n",
        "\n",
        "| Transaction   | Items                                       |\n",
        "|:--------------|:--------------------------------------------|\n",
        "| t1            | {T-shirt, Trousers, Belt}                   |\n",
        "| t2            | {T-shirt, Jacket}                           |   \n",
        "| t3            | {Jacket, Gloves}                            |  \n",
        "| t4            | {T-shirt, Trousers, Jacket}                 |\n",
        "| t5            | {T-shirt, Trousers, Sneakers, Jacket, Belt} |   \n",
        "| t6            | {Trousers, Sneakers, Belt}                  |\n",
        "| t7            | {Trousers, Belt, Sneakers}                  |\n",
        "\n",
        "</center>  \n",
        "\n",
        "Na tabela acima, podemos ver sete transações de uma loja de roupas. Cada transação mostra os itens comprados naquela transação. Podemos representar nossos itens como um conjunto de itens da seguinte maneira:\n",
        "\n",
        "$$I=\\{i_1, i_2,..., i_k\\}$$\n",
        "\n",
        "\n",
        "No nosso caso, isso corresponde a:\n",
        "\n",
        "$$I=\\{T\\text- shirt, Trousers, Belt, Jacket, Gloves, Sneakers\\}$$\n",
        "\n",
        "A **transaction** is represented by the following expression:\n",
        "\n",
        "$$T=\\{t_1, t_2,..., t_n\\}$$\n",
        "\n",
        "Por exemplo,\n",
        "\n",
        "$$t_1=\\{T\\text- shirt, Trousers, Belt\\}$$\n",
        "\n",
        "Então, uma **regra de associação** é definida como uma implicação da forma:\n",
        "\n",
        "<center> $X \\Rightarrow Y$, where $X \\subset I$, $Y \\subset I$ and $X \\cap Y = 0$ </center>\n",
        "\n",
        "Por exemplo,\n",
        "\n",
        "$$\\{T\\text- shirt, Trousers\\} \\Rightarrow \\{Belt\\}$$\n",
        "\n",
        "Nas seções seguintes, vamos definir quatro métricas para medir a precisão de uma regra:\n",
        "\n",
        "## Support\n",
        "\n",
        "O Suporte é uma indicação de com que frequência o conjunto de itens aparece no conjunto de dados.\n",
        "\n",
        "$$supp(X \\Rightarrow Y)=\\dfrac{|X \\cup Y|}{n}$$\n",
        "\n",
        "Em outras palavras, é o número de transações com ambos $X$ e $Y$ dividido pelo número total de transações. As regras não são úteis para valores baixos de suporte.\n",
        "\n",
        "## Confidence\n",
        "\n",
        "Para uma regra $X \\Rightarrow Y$, a confiança mostra a porcentagem em que $Y$ é comprado com $X$. É uma indicação de com que frequência a regra foi considerada verdadeira.\n",
        "\n",
        "$$conf(X \\Rightarrow Y)=\\dfrac{supp(X \\cup Y)}{supp(X)}$$\n",
        "\n",
        "Por exemplo, a regra $T\\text- shirt \\Rightarrow Trousers$ tem uma confiança de 3/4, o que significa que em 75% das transações que contêm uma camiseta, a regra é correta (75% das vezes que um cliente compra uma camiseta, também compra calças).\n",
        "\n",
        "## Lift\n",
        "\n",
        "O lift de uma regra é a razão entre o suporte observado e o suporte esperado se $X$ e $Y$ fossem independentes, e é definido como:\n",
        "\n",
        "$$lift(X \\Rightarrow Y)=\\dfrac{supp(X \\cup Y)}{supp(X)supp(Y) }$$\n",
        "\n",
        "\n",
        "Valores de lift maiores indicam associações mais fortes.\n",
        "\n",
        "<!-- ## Conviction\n",
        "\n",
        "A convicção de uma regra é definida como:\n",
        "\n",
        "$$conv(X \\Rightarrow Y)=\\dfrac{1-supp(Y)}{1-conf(X \\Rightarrow Y) }$$\n",
        "\n",
        "Pode ser interpretada como a razão entre a frequência esperada de ocorrência de $X$ sem $Y$, se $X$ e $Y$ fossem independentes, dividida pela frequência observada de previsões incorretas. Um valor alto significa que o consequente depende fortemente do antecedente. -->\n",
        "\n",
        "Se você deseja obter mais informações sobre essas medidas, por favor, consulte [aqui](https://en.wikipedia.org/wiki/Association_rule_learning)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvsONiexUv1B"
      },
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lrZIDY3uiUa",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#Mlxtend (machine learning extensions) is a Python library of useful tools for the day-to-day data science tasks.\n",
        "!pip install mlxtend\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SM9rP9QIV9uB",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# we need to install mlxtend on anaconda prompt by typing 'pip install mlxtend'\n",
        "from mlxtend.frequent_patterns import apriori\n",
        "from mlxtend.frequent_patterns import association_rules\n",
        "\n",
        "# graph\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9H_8rizLztk"
      },
      "source": [
        "# Loading Data\n",
        "\n",
        "> We will use a dataset containing transactions from a bakery to apply the algorithm and find combinations of products that are purchased together. Let's start!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKCoPVuzWh0Z",
        "tags": []
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/GabrieleAraujo/database-data-mining/main/BreadBasket_DMS.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ms4hcFl_YkEY",
        "tags": []
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9Q9FS-oa_HZ"
      },
      "source": [
        "# Cleaning the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WSfWlfsbfwq"
      },
      "source": [
        "Each row in the table above represents an item in a transaction. Some rows may contain missing values. Let's eliminate these lines!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skN6AZI5UeC6",
        "tags": []
      },
      "outputs": [],
      "source": [
        "df.isnull().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTBdw6jTOXcY"
      },
      "outputs": [],
      "source": [
        "df.loc[df['Item']=='NONE', :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "br6MBSsRbIqy",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def cleanup_dataset(df):\n",
        "    # Returns new dataset without NONE values in specified c\n",
        "    df_none_entries = df.loc[df['Item']=='NONE',:]\n",
        "    return df.drop(df_none_entries.index)\n",
        "\n",
        "dataset = cleanup_dataset(df)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6eTyEZOTc-U"
      },
      "source": [
        "We might want to focus in on each of these aspects seperatly. So let's make a list of transactions and a seperate list of items!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8yshrfkTcTK",
        "tags": []
      },
      "outputs": [],
      "source": [
        "list_of_transactions = dataset[['Transaction', 'Date', 'Time']].drop_duplicates()\n",
        "list_of_transactions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpZQsk1bbvw4"
      },
      "source": [
        "# Setting up some auxillary functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DN2Dv7Bb0HC"
      },
      "source": [
        "Here are a few functions that will make it easier for us to interact with our dataframes:\n",
        "\n",
        "The Date and Time headings encode many different pieces of information. Splitting up this information is gonna make it easier for us to group data points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-wLi9TLcJpA"
      },
      "source": [
        "## Date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXGCE3KAb2sS",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def split_date_field(df_orig):\n",
        "    \"\"\"\n",
        "    Converts the Date Column into three separate columns\n",
        "    (YYYY-MM-DD) -> (YYYY, MM, DD)\n",
        "    \"\"\"\n",
        "    df = df_orig.copy()\n",
        "    date = pd.to_datetime(df['Date'])\n",
        "\n",
        "    df['Year'] = date.dt.year\n",
        "    df['Month'] = date.dt.month\n",
        "    df['Day'] = date.dt.day\n",
        "\n",
        "    month_map = {\n",
        "        1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr',\n",
        "        5: 'May', 6: 'Jun', 7: 'Jul', 8: 'Aug',\n",
        "        9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'\n",
        "    }\n",
        "    df['Month'] = df['Month'].map(month_map)\n",
        "\n",
        "    weekday_map = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
        "    df['Weekday'] = date.dt.weekday.apply(lambda x: weekday_map[x])\n",
        "\n",
        "    return df\n",
        "df_with_dates = split_date_field(dataset)\n",
        "df_with_dates.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdjvVzUpcLkI"
      },
      "source": [
        "## Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjvSPdNDcNXI",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def split_time_field(df_orig):\n",
        "    \"\"\"\n",
        "    Converts the Date Column into three sepreate columns\n",
        "\n",
        "    (HH-MM-SS) -> (HH, MM, SS)\n",
        "    \"\"\"\n",
        "    df = df_orig.copy()\n",
        "    df['Hours'], df['Mins'], df['Secs'] = df['Time'].str.split(':').str\n",
        "    return df\n",
        "\n",
        "df_with_time = split_time_field(dataset)\n",
        "df_with_time.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK8ezFYCdJfQ"
      },
      "source": [
        "# Understanding the data\n",
        "Now we can use the data to answer some real questions!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sguirxyTdM54"
      },
      "source": [
        "## How many transactions took place?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTv83Kw8dRmR",
        "tags": []
      },
      "outputs": [],
      "source": [
        "dataset['Transaction'].nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQgj1-cZddOw"
      },
      "source": [
        "## How many items are in the basket?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mb_L3Xv2dfJ4",
        "tags": []
      },
      "outputs": [],
      "source": [
        "dataset['Item'].nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbXfiHjqd5T5"
      },
      "source": [
        "## What are the most popular items?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CImmk8M9d73J",
        "tags": []
      },
      "outputs": [],
      "source": [
        "transaction_count = dataset.groupby(by='Item')[['Transaction']].count().sort_values(by='Transaction', ascending=False)\n",
        "transaction_count.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbYKwTfbeRmq",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def convert_to_percentage(x):\n",
        "    return 100 * x / float(x.sum())\n",
        "\n",
        "transaction_percentage = transaction_count.apply(convert_to_percentage)\n",
        "transaction_percentage.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5cmY9WHp9F1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x='Transaction', y=transaction_count.index[:20], data=transaction_count.head(20), palette='viridis')\n",
        "plt.title('Most popular line items')\n",
        "plt.xlabel('Transaction Count')\n",
        "plt.ylabel('Item')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvC9_XrwZstD"
      },
      "source": [
        "# Time distribution Transactions\n",
        "Let’s display some other visualizations describing the time distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIrL1OEQaZjc"
      },
      "source": [
        "## Transactions per day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_Lf9nX-jZja",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# To correct the order in graphs\n",
        "order_month = ('Jan', 'Feb', 'Mar','Apr','Oct', 'Nov', 'Dec')\n",
        "order_weekday = ('Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiPsQ8O13Nfg",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Count of items per day (month and day of the week), with ordering of the days of the week\n",
        "item_count_by_day = (df_with_dates.groupby(['Month', 'Weekday']).size().reset_index(name='Transaction'))\n",
        "item_count_by_day['Weekday'] = pd.Categorical(item_count_by_day['Weekday'], categories=order_weekday, ordered=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtZ8jWFksms6",
        "tags": []
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='Month', y='Transaction', hue='Weekday', data=item_count_by_day, palette = 'Paired', order = order_month )\n",
        "plt.title('Transactions per day')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Transactions')\n",
        "plt.legend(title='Weekday', bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LJ41BHfbstD"
      },
      "source": [
        "The data set includes dates from 30/10/2016 to 09/04/2017, that’s why we have so few transactions in October and April."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu2oc9XBbJxC"
      },
      "source": [
        "## Transactions per Month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bQCyyUEiVaa",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Unique count of transactions per month, sorted in descending order\n",
        "count_month = df_with_dates.groupby('Month')['Transaction'].nunique().sort_values(ascending=False).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zL6zDpLf2ODj",
        "tags": []
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "ax = sns.barplot(x='Month', y='Transaction', data=count_month, palette='crest', order = order_month)\n",
        "plt.title('Transaction per month')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Transactions')\n",
        "\n",
        "for i in ax.containers:\n",
        "    ax.bar_label(i)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OX2gQ8Ctkbfh"
      },
      "source": [
        "## Transactions per weekday"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQXBoUS4kfLA",
        "tags": []
      },
      "outputs": [],
      "source": [
        "count_weekday = df_with_dates.groupby('Weekday')['Transaction'].nunique().sort_values(ascending=False).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESKp5U6XITNU",
        "tags": []
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "ax = sns.barplot(x='Weekday', y='Transaction', data=count_weekday, palette='crest', order = order_weekday)\n",
        "plt.title('Transaction per weekday')\n",
        "plt.xlabel('Weekday')\n",
        "plt.ylabel('Transactions')\n",
        "\n",
        "for i in ax.containers:\n",
        "    ax.bar_label(i)\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1bGtyWzl8_q"
      },
      "source": [
        "As we can see, Saturday is the busiest day in the bakery. Conversely, Wednesday is the day with fewer transactions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnbytSYAllgD"
      },
      "source": [
        "## Transactions per hour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wu9nH_fZlmhh",
        "tags": []
      },
      "outputs": [],
      "source": [
        "df_with_time.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nScEKBTboUpB",
        "tags": []
      },
      "outputs": [],
      "source": [
        "count_hours = df_with_time.groupby('Hours')['Transaction'].nunique().sort_values(ascending=False).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RPcs5kzohrm",
        "tags": []
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(9, 6))\n",
        "\n",
        "ax = sns.barplot(x='Hours', y='Transaction', data=count_hours.sort_values(by='Hours'), palette='viridis_r')\n",
        "plt.title('Transaction per hours')\n",
        "plt.xlabel('Hour')\n",
        "plt.ylabel('Transactions')\n",
        "\n",
        "for i in ax.containers:\n",
        "    ax.bar_label(i)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P0pjlwyLidC"
      },
      "source": [
        "# Apriori algorithm\n",
        "\n",
        "O algoritmo Apriori gera regras de associação para um conjunto de dados específico. Uma regra de associação implica que se um item A ocorre, então o item B também ocorre com uma certa probabilidade.\n",
        "\n",
        "O primeiro passo para criar um conjunto de regras de associação é determinar os limiares ideais para suporte e confiança. Se definirmos esses valores muito baixos, o algoritmo levará mais tempo para ser executado e obteremos muitas regras (a maioria delas não será útil). Então, quais valores escolher? Podemos experimentar diferentes valores de suporte e confiança e visualizar graficamente quantas regras são geradas para cada combinação.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMQSaJQtsVAw"
      },
      "source": [
        "## Starting preparation of df for receiving product association"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ_JAF6PILhO"
      },
      "source": [
        "We will create a unique representation of purchased products, where each row is a transaction, each column is a product, and the values indicate the quantity of each product in a transaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ythXBYtpwfBu",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# One-hot representation of products purchased\n",
        "dataset['Quantity'] = 1\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "basket = dataset.groupby(['Transaction', 'Item'])['Quantity'].sum().unstack().fillna(0)\n",
        "basket"
      ],
      "metadata": {
        "id": "1MFBLCLLsb0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIePvVqCzWP4"
      },
      "source": [
        "There are a lot of zeros in the data but we also need to make sure any positive values are converted to a 1 and anything less the 0 is set to 0. This step will complete the one hot encoding of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yb2WDBjqtiFB",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Define a function to encode values into binary (0 or 1)\n",
        "def encode_units(x):\n",
        "    if x <= 0:\n",
        "        return 0\n",
        "    if x >= 1:\n",
        "        return 1\n",
        "\n",
        "basket_sets = basket.applymap(encode_units)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "basket_sets"
      ],
      "metadata": {
        "id": "kgyK_88atGLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvn5H0iNV-un"
      },
      "source": [
        "We can try different values of support and confidence and see graphically how many rules are generated for each combination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4THP5H8PCvA",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def apriori_confidence_levels(data, support_levels, confidence_levels):\n",
        "    rule_counts = []\n",
        "\n",
        "    for support_level in support_levels:\n",
        "        current_rule_counts = []\n",
        "        for confidence_level in confidence_levels:\n",
        "            rules = association_rules(apriori(data, min_support=support_level, use_colnames=True), metric='confidence', min_threshold=confidence_level)\n",
        "            current_rule_counts.append(len(rules))\n",
        "        rule_counts.append(current_rule_counts)\n",
        "\n",
        "    return rule_counts\n",
        "\n",
        "def plot_confidence_levels(support_levels, confidence_levels, rule_counts):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    for i, support_level in enumerate(support_levels):\n",
        "        plt.plot(confidence_levels, rule_counts[i], marker='o', label=f'Support Level {support_level*100}%')\n",
        "\n",
        "    plt.title(\"Apriori Algorithm with Different Confidence Levels for Each Support Level\")\n",
        "    plt.xlabel('Confidence Level')\n",
        "    plt.ylabel('Number of Rules')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "support_levels = [0.1, 0.05, 0.01, 0.005]\n",
        "confidence_levels = [1.0, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]\n",
        "\n",
        "rule_counts = apriori_confidence_levels(basket_sets, support_levels, confidence_levels)\n",
        "plot_confidence_levels(support_levels, confidence_levels, rule_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTfOMi-WSGOT"
      },
      "source": [
        "Support level of 1%. We started to get dozens of rules, of which 13 have a confidence of at least 50%. To sum up, we are going to use a support level of 1% and a confidence level of 50%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLwefpE00IL4"
      },
      "source": [
        "We can change the min_support value from 0 to 1, default value if 0.5, but as our support values are less than 0.5, to include more datasets. According to the graph above, the values of min_support = 0.01 and min_threshold=0.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNKS-BSfz3TQ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# The result is a table that lists the frequent sets and their relative frequency ('support').\n",
        "frequent_itemsets = apriori(basket_sets, min_support=0.01, use_colnames=True)\n",
        "frequent_itemsets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2LYwUUqS6WX",
        "tags": []
      },
      "outputs": [],
      "source": [
        "rules_sup1_conf50 = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.5)\n",
        "rules_sup1_conf50"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing the rules distribution color mapped"
      ],
      "metadata": {
        "id": "Qy7TWBkSjSg0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4ne5Aj-wbsO",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Visualizing the rules distribution color mapped by Lift\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(rules_sup1_conf50['support'], rules_sup1_conf50['lift'], c=rules_sup1_conf50['confidence'], cmap='YlOrRd')\n",
        "plt.title('Distribution of the first 10 association rules with a lift-based color map')\n",
        "plt.xlabel('Support')\n",
        "plt.ylabel('Lift')\n",
        "plt.colorbar(label = 'Confidence')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu6VhCRWiAaN"
      },
      "source": [
        "Based on the graph we have one or two points of interest with higher lift than the surrounding points:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9cwAo8fTLqU",
        "tags": []
      },
      "outputs": [],
      "source": [
        "rules_sup1_conf50[['antecedents', 'consequents', 'support', 'confidence', 'lift']].sort_values('lift', ascending=False).head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iS0pNUc1wV48",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Visualizing the rules distribution color mapped by confidence\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(rules_sup1_conf50['support'].head(13), rules_sup1_conf50['confidence'].head(13), c=rules_sup1_conf50['lift'].head(13), cmap='YlOrRd')\n",
        "plt.title('Distribution of the first 10 association rules with a confidence-based color map')\n",
        "plt.xlabel('Support')\n",
        "plt.ylabel('Confidence')\n",
        "plt.colorbar(label = 'Lift')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Association between antecedents and consequents"
      ],
      "metadata": {
        "id": "f1xsIiD_jk-_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRR5JFmsaC4W"
      },
      "source": [
        "Network graph to check the association between antecedents and consequents obtained after the association rule.\n",
        "\n",
        "Font: https://plotly.com/python/network-graphs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9CHcOmxZhEu",
        "tags": []
      },
      "outputs": [],
      "source": [
        "GA = nx.from_pandas_edgelist(rules_sup1_conf50, source='antecedents', target='consequents')\n",
        "\n",
        "node_labels = {node: ', '.join(map(str, node)) for node in GA.nodes()}\n",
        "fig, ax = plt.subplots(figsize=(10, 4))\n",
        "nx.draw(GA, with_labels=True, labels=node_labels, font_size=10, font_color='black', node_size=1000, node_color='skyblue', ax=ax)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jErhw8Wfa-Q"
      },
      "source": [
        "Graph of Association Rules with Support and Lift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0IdH5ahfk5e",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# based on lift\n",
        "filtered_rules = rules_sup1_conf50[rules_sup1_conf50['lift'] > 1.0]\n",
        "GA1 = nx.from_pandas_edgelist(filtered_rules, source='antecedents', target='consequents', create_using=nx.DiGraph())\n",
        "GA = nx.relabel_nodes(GA1, {node: ', '.join(map(str, node)) for node in GA1.nodes()})\n",
        "\n",
        "# support\n",
        "node_sizes = [filtered_rules[filtered_rules['antecedents'] == set(node)]['support'].values[0] * 10000\n",
        "              if len(filtered_rules[filtered_rules['antecedents'] == set(node)]) > 0 else 0 for node in GA1.nodes]\n",
        "# lift\n",
        "edge_colors = [filtered_rules[(filtered_rules['antecedents'] == set(edge[0])) & (filtered_rules['consequents'] == set(edge[1]))]['lift'].values[0]\n",
        "               if len(filtered_rules[(filtered_rules['antecedents'] == set(edge[0])) & (filtered_rules['consequents'] == set(edge[1]))]) > 0 else 0 for edge in GA1.edges]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "nx.draw(GA, with_labels=True, font_size=12, font_color='black', node_size=node_sizes, node_color='skyblue',\n",
        "        edge_color=edge_colors, edge_cmap=plt.cm.coolwarm, ax=ax, width=1, edgecolors='black')\n",
        "legend_handle = ax.scatter([], [], s=1000, color='skyblue', label='Support')\n",
        "\n",
        "sm = plt.cm.ScalarMappable(cmap=plt.cm.coolwarm, norm=plt.Normalize(vmin=min(edge_colors), vmax=max(edge_colors)))\n",
        "sm.set_array([])\n",
        "cbar = plt.colorbar(sm)\n",
        "cbar.set_label('Lift')\n",
        "\n",
        "plt.legend(handles=[legend_handle], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gN20EWogh9D9"
      },
      "source": [
        "# Exercícios\n",
        "*   O que acontece quando diminuímos o nível de suporte? Por que?\n",
        "*   O que acontece quando aumentamos o nível de confiança? Por que?\n",
        "*   Como interpretar os resultados em termos de lift e qual seria um valor \"bom\" para o lift em um contexto específico?\n",
        "*   Usando o conjunto de dados anterior, execute o algoritmo Apriori com um nível de suporte de 5% e um nível de confiança de 10%. As regras são interessantes? Por que?\n",
        "*   Execute também o algoritmo Apriori com um nível de suporte de 0.5% e um nível de confiança de 60%. As regras são interessantes? Por que?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Outros datasets para Market Basket Analysis/Association rules\n",
        "\n",
        "\n",
        "*   [Groceries dataset](https://www.kaggle.com/datasets/heeraldedhia/groceries-dataset/data)\n",
        "*   [The Bread Basket\n",
        "](https://www.kaggle.com/datasets/mittalvasu95/the-bread-basket)\n",
        "*   [E-commerce Customer Data For Behavior Analysis](https://www.kaggle.com/datasets/shriyashjagtap/e-commerce-customer-for-behavior-analysis)\n",
        "*  [Market_Basket_Optimisation](https://www.kaggle.com/datasets/andrewtoh78/market-basket-optimisation)\n",
        "*  [Video Streaming Platforms](https://www.kaggle.com/datasets/utkarshsen/video-streaming-platforms/data)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7E0HWMz_jwZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Outros algoritmos usados em Market Basket Analysis:\n",
        "*  Frequent Pattern Growth (FP-Growth);\n",
        "*  Equivalence Class Transformation (EClaT);\n",
        "\n",
        "\n",
        "\n",
        "Para mais detalhes dos algoritmos:\n",
        "- *Chee, C. H., Jaafar, J., Aziz, I. A., Hasan, M. H., & Yeoh, W. (2019). Algorithms for frequent itemset mining: a literature review. Artificial Intelligence Review, 52, 2603-2621.* Disponível aqui: https://link.springer.com/article/10.1007/s10462-018-9629-z;\n",
        "\n",
        "- [Minerações de dados frequentes com Apriori e FP Growth](https://medium.com/@abnersuniga7/encontre-padrões-nos-seus-dados-com-apriori-e-fp-growth-4a581ec1b22)\n",
        "\n"
      ],
      "metadata": {
        "id": "FHZgSicjjHjJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referências\n",
        "\n",
        "- Gupta, M. K., & Chandra, P. (2020). A comprehensive survey of data mining. International Journal of Information Technology, 12(4), 1243-1257.\n",
        "\n",
        "- Han, J., Pei, J., & Tong, H. (2022). Data mining: concepts and techniques. Morgan kaufmann.\n",
        "\n",
        "- Shmueli, G., Bruce, P. C., Gedeck, P., & Patel, N. R. (2019). Data mining for business analytics: concepts, techniques and applications in Python. John Wiley & Sons.\n",
        "\n",
        "- Kurnia, Y., Isharianto, Y., Giap, Y. C., & Hermawan, A. (2019, March). Study of application of data mining market basket analysis for knowing sales pattern (association of items) at the O! Fish restaurant using apriori algorithm. In Journal of Physics: Conference Series (Vol. 1175, No. 1, p. 012047). IOP Publishing.\n",
        "\n",
        "- Ünvan, Y. A. (2021). Market basket analysis with association rules. Communications in Statistics-Theory and Methods, 50(7), 1615-1628."
      ],
      "metadata": {
        "id": "UkwzDAO5cSeg"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}